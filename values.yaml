global:
  sidecar:
    dashboard: grafana_dashboard

victoria-metrics-single:
  enabled: true
  server:
    # -- Data retention period in month https://github.com/VictoriaMetrics/VictoriaMetrics/issues/17 5d,w,y
    retentionPeriod: 1
    extraArgs:
      envflag.enable: "true"
      envflag.prefix: VM_
      loggerFormat: json
      dedup.minScrapeInterval: 60s
    image:
      tag: v1.61.1
    resources:
      limits:
        cpu: 800m
        memory: 2Gi
      requests:
        cpu: 100m
        memory: 500Mi
    persistentVolume:
      enabled: true
      accessModes:
        - ReadWriteOnce
      storageClass: "k8s-storage"
      # -- Size of the volume. Better to set the same as resource limit memory property.
      size: 70Gi

kube-prometheus-stack:
  grafana:
    enabled: true
    image:
      repository: grafana/grafana
      tag: 8.0.4
    plugins:
      - digrich-bubblechart-panel
      - grafana-clock-panel
      - grafana-polystat-panel
      - yesoreyeram-boomtable-panel
      - natel-discrete-panel
      - digiapulssi-breadcrumb-panel
    grafana.ini:
      server:
        root_url: "%(protocol)s://%(domain)s/grafana/"
        serve_from_sub_path: true
    ## Deploy default dashboards.
    ##
    defaultDashboardsEnabled: true
    adminPassword: grafanaAdmin1!

    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'extra'
            orgId: 1
            folder: 'Extra'
            type: file
            disableDeletion: true
            editable: true
            options:
              path: /var/lib/grafana/dashboards/extra

    dashboards:
      extra:
        prometheus-stats:
          gnetId: 3662
          datasource: Prometheus
        kminion-cluster:
          gnetId: 14012
          datasource: Prometheus
        kminion-consumers:
          gnetId: 14014
          datasource: Prometheus
        kminion-topics:
          gnetId: 14013
          datasource: Prometheus
        zookeeper:
          gnetId: 11442
          datasource: Prometheus
        micrometer:
          gnetId: 4701
          datasource: Prometheus
        elasticsearch:
          gnetId: 2322
          datasource: Prometheus
        kubernetes-monitoring:
          gnetId: 315
          datasource: Prometheus
        kubernetes-node-full:
          gnetId: 1860
          datasource: Prometheus
        kubernetes-apache-nifi:
          gnetId: 12314
          datasource: Prometheus
    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: kong
        konghq.com/strip-path: "true"
      path: /grafana
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard

  prometheusOperator:
    image:
      tag: v0.47.0

  prometheus:
    prometheusSpec:
      replicas: 1
      resources:
        limits:
          cpu: 800m
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 100Mi
      routePrefix: /prometheus
  additionalPrometheusRulesMap:
    rule-name:
      groups:
        - name: datastore-leonardo.rules
          rules:
            - alert: MongodbDown
              expr: (absent(mongodb_up)) or (mongodb_up == 0)
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: MongoDB Down (instance {{ $labels.instance }})
                description: "MongoDB instance is down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: MongodbTooManyConnections
              expr: avg by(instance) (rate(mongodb_connections{state="current"}[1m])) / avg by(instance) (sum (mongodb_connections) by (instance)) * 100 > 80
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: MongoDB too many connections (instance {{ $labels.instance }})
                description: "Too many connections (> 80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: ElasticsearchClusterRed
              annotations:
                description:  "Elastic Cluster Red status\n "
                summary: "Elasticsearch Cluster Red"
              expr: (elasticsearch_cluster_health_status{color="red"} == 1) or (absent(elasticsearch_cluster_health_status))
              for: 1m
              labels:
                severity: critical
            - alert: ElasticsearchClusterYellow
              annotations:
                description:  "Elastic Cluster Yellow status\n "
                summary: "Elasticsearch Cluster Yellow"
              expr: elasticsearch_cluster_health_status{color="yellow"} == 1
              for: 1m
              labels:
                severity: warning
            - alert: ElasticsearchHeapUsageTooHigh
              expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 90
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Elasticsearch Heap Usage Too High (instance {{ $labels.instance }})
                description: "The heap usage is over 90%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: ElasticsearchDiskOutOfSpace
              annotations:
                description:  "The disk usage is over 90%  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                summary: "Elasticsearch disk out of space "
              expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10
              for: 1m
              labels:
                severity: error
            - alert: ZookeeperDown
              expr: (absent(zk_up)) or (zk_up == 0)
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: Zookeeper Down (instance {{ $labels.instance }})
                description: "Zookeeper down on instance {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: ZookeeperNotOk
              expr: zk_ruok == 0
              for: 3m
              labels:
                severity: error
              annotations:
                summary: Zookeeper Not Ok (instance {{ $labels.instance }})
                description: "Zookeeper instance is not ok\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KongNotReachable
              expr: kong_datastore_reachable == 0
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: Kong not reachable (namespace {{ $labels.namespace }})
                description: "Kong not reachable on namespace {{ $labels.namespace }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: KafkaDown
              expr: (absent(kminion_kafka_cluster_info)) or (kminion_kafka_cluster_info == 0)
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: Kafka down(instance {{ $labels.instance }})
                description: "Kafka down on instance {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

prometheus-pushgateway:
  enabled: true
  # Default values for prometheus-pushgateway.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  # Provide a name in place of prometheus-pushgateway for `app:` labels
  nameOverride: "prom-aggregation-gateway"

  # Provide a name to substitute for the full names of resources
  fullnameOverride: "prom-aggregation-gateway"

  image:
    repository: weaveworks/prom-aggregation-gateway
    tag: master-c4415bbe
    pullPolicy: IfNotPresent

  # Optional pod imagePullSecrets
  imagePullSecrets: [ ]

  service:
    type: NodePort
    port: 9091
    targetPort: 9091

    # Optional - Can be used for headless if value is "None"
    clusterIP: ""

    loadBalancerIP: ""
    loadBalancerSourceRanges: [ ]

  # Optional pod annotations
  podAnnotations: { }

  # Optional pod labels
  podLabels: { }

  # Optional service annotations
  serviceAnnotations: { }

  # Optional service labels
  serviceLabels: { }

  # Optional serviceAccount labels
  serviceAccountLabels: { }

  # Optional persistentVolume labels
  persistentVolumeLabels: { }

  # Optional additional environment variables
  extraVars:

  ## Additional pushgateway container arguments
  ##
  ## example:
  ## extraArgs:
  ##  - --persistence.file=/data/pushgateway.data
  ##  - --persistence.interval=5m
  extraArgs: [ "-listen", ":9091" ]

  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 200m
    #    memory: 50Mi
    # requests:
    #   cpu: 100m
    #   memory: 30Mi

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:

  ## Configure ingress resource that allow you to access the
  ## pushgateway installation. Set up the URL
  ## ref: http://kubernetes.io/docs/user-guide/ingress/
  ##
  ingress:
    ## Enable Ingress.
    ##
    enabled: false
    # AWS ALB requires path of /*
    path: /

      ## Annotations.
      ##
      # annotations:
      #   kubernetes.io/ingress.class: nginx
      #   kubernetes.io/tls-acme: 'true'

  tolerations: { }
  # - effect: NoSchedule
  #   operator: Exists

  ## Node labels for pushgateway pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: { }

  replicaCount: 1
  ## Security context to be added to push-gateway pods
  ##
  securityContext:
    fsGroup: 65534
    runAsUser: 65534
    runAsNonRoot: true

  ## Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  # Enable this if you're using https://github.com/coreos/prometheus-operator
  serviceMonitor:
    enabled: true
    namespace: monitoring

    # Fallback to the prometheus default unless specified
    # interval: 10s

    # Fallback to the prometheus default unless specified
    # scrapeTimeout: 30s

    ## Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
    additionalLabels:
      release: prometheus

    # Retain the job and instance labels of the metrics pushed to the Pushgateway
    # [Scraping Pushgateway](https://github.com/prometheus/pushgateway#configure-the-pushgateway-as-a-target-to-scrape)
    honorLabels: true

    ## Metric relabel configs to apply to samples before ingestion.
    ## [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)
    metricRelabelings: [ ]
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## Relabel configs to apply to samples before ingestion.
    ## [Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config)
    relabelings: [ ]
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

  # The values to set in the PodDisruptionBudget spec (minAvailable/maxUnavailable)
  # If not set then a PodDisruptionBudget will not be created
  podDisruptionBudget: { }

  priorityClassName:

  # Deployment Strategy type
  strategy:
    type: Recreate

  persistentVolume:
  ## If true, pushgateway will create/use a Persistent Volume Claim
  ## If false, use emptyDir
    enabled: true

    ## pushgateway data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## pushgateway data Persistent Volume Claim annotations
    ##
    annotations: { }
    ## pushgateway data Persistent Volume existing claim name
    ## Requires pushgateway.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## pushgateway data Persistent Volume mount root path
    ##
    mountPath: /data

    ## pushgateway data Persistent Volume size
    ##
    size: 2Gi
      ## pushgateway data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
    storageClass: "aws-efs"

    ## Subdirectory of pushgateway data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

    # Configuration for clusters with restrictive network policies in place:
    # - allowAll allows access to the PushGateway from any namespace
    # - customSelector is a list of pod/namespaceSelectors to allow access from
    # These options are mutually exclusive and the latter will take precedence.
  networkPolicy: { }
      # allowAll: true
      # customSelectors:
      #   - namespaceSelector:
      #       matchLabels:
      #         type: admin
      #   - podSelector:
    #       matchLabels:
    #         app: myapp

